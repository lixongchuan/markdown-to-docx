import io, datetime, json
from flask import Flask, request, send_file, render_template, jsonify, Response, stream_with_context
from services.docx_service import DocxGenerator
from openai import OpenAI, APIError

app = Flask(__name__)
# --- ğŸš€ é…ç½®åŒºï¼šæ¨¡å‹è‡ªåŠ¨åˆ‡æ¢ç­–ç•¥ ---
# ä½ çš„ ModelScope Token
API_KEY = "ms-ee053112-72cc-4ee7-8e05-4437e1cde575"
BASE_URL = "https://api-inference.modelscope.cn/v1"

# ğŸ“‹ æ¨¡å‹å¤‡é€‰åˆ—è¡¨ (ä¼˜å…ˆçº§ä»ä¸Šåˆ°ä¸‹)
# å½“ç¬¬ä¸€ä¸ªæŠ¥é”™æ—¶ï¼Œè‡ªåŠ¨å°è¯•ç¬¬äºŒä¸ªï¼Œä»¥æ­¤ç±»æ¨
MODEL_LIST = [
    "Qwen/Qwen3-Next-80B-A3B-Instruct",
    "Qwen/Qwen3-VL-30B-A3B-Instruct",         # é¦–é€‰ï¼šä½ æŒ‡å®šçš„è§†è§‰å¢å¼ºç‰ˆ
    "Qwen/Qwen3-30B-A3B-Instruct-2507",       # å¤‡é€‰1ï¼šé€šä¹‰åƒé—® 2.5 72B (ç›®å‰æœ€å¼ºå¼€æºä¹‹ä¸€)
    "Qwen/Qwen3-32B",                         # å¤‡é€‰2ï¼š32B (é€Ÿåº¦ä¸è´¨é‡å¹³è¡¡)
    "Qwen/Qwen2.5-7B-Instruct",               # å¤‡é€‰3ï¼š7B (é€Ÿåº¦æå¿«ï¼Œä¿åº•)
]

# åˆå§‹åŒ– Client 
client = OpenAI(
    api_key=API_KEY,
    base_url=BASE_URL
)

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/convert', methods=['POST'])
def convert():
    try:
        md_text = request.form.get('markdown', '').strip()
        if not md_text:
            return jsonify({'error': 'å†…å®¹ä¸ºç©º'}), 400

        # ä» form æå–æ‰€æœ‰é…ç½®
        config = request.form.to_dict()
        
        # è°ƒç”¨æœåŠ¡å±‚
        generator = DocxGenerator(md_text, config)
        doc = generator.generate()

        file_stream = io.BytesIO()
        doc.save(file_stream)
        file_stream.seek(0)

        filename = f'Markdown_Export_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.docx'
        return send_file(
            file_stream, 
            as_attachment=True, 
            download_name=filename,
            mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        )
    except Exception as e:
        app.logger.error(f"Error: {e}")
        return jsonify({'error': str(e)}), 500

# é…ç½® OpenAI Client
client = OpenAI(
    api_key="ms-ee053112-72cc-4ee7-8e05-4437e1cde575", 
    base_url="https://api-inference.modelscope.cn/v1/"
)

@app.route('/chat_stream', methods=['POST'])
def chat_stream():
    # è·å–å‰ç«¯å†å²è®°å½•
    data = request.get_json()
    messages = data.get('messages', [])
    
    # ç³»ç»Ÿæç¤ºè¯è¡¥å…¨
    if not messages or messages[0].get('role') != 'system':
        messages.insert(0, {
            'role': 'system', 
            'content': 'You are a helpful Markdown assistant.'
        })

    def generate():
        # ğŸ”„ æ ¸å¿ƒé€»è¾‘ï¼šæ¨¡å‹è½®è¯¢é‡è¯•
        for model_name in MODEL_LIST:
            try:
                # print(f"Trying model: {model_name}...") # è°ƒè¯•ç”¨
                
                response = client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    stream=True
                )
                
                # å¼€å§‹æµå¼ä¼ è¾“
                # æ³¨æ„ï¼šä¸€æ—¦å¼€å§‹ yield æ•°æ®ï¼Œè¯´æ˜è¿æ¥æˆåŠŸï¼Œå°±ä¸å†åˆ‡æ¢æ¨¡å‹äº†
                # é™¤éæ˜¯åœ¨ yield è¿‡ç¨‹ä¸­æ–­å¼€ï¼ˆè¾ƒå°‘è§ï¼Œé€šå¸¸æ˜¯è¿æ¥æ—¶æŠ¥é”™ï¼‰
                for chunk in response:
                    if chunk.choices and chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
                
                # å¦‚æœä»£ç èƒ½èµ°åˆ°è¿™é‡Œï¼Œè¯´æ˜æ•´ä¸ªæµä¼ è¾“æˆåŠŸå®Œæˆ
                return 

            except Exception as e:
                # æ•è·é”™è¯¯ï¼Œæ‰“å°æ—¥å¿—
                print(f"âŒ Model [{model_name}] failed: {str(e)}")
                
                if model_name == MODEL_LIST[-1]:
                    yield f"Error: All models are busy or quota exceeded. Last error: {str(e)}"
                else:
                    # è¿˜æœ‰å¤‡ç”¨æ¨¡å‹ï¼Œç»§ç»­å¾ªç¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ª
                    continue

    return Response(stream_with_context(generate()), content_type='text/plain; charset=utf-8')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)